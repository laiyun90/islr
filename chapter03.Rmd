---
title: "Chapter 3: Linear Regression"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  github_document
bibliography: references.bib
csl: apa.csl
nocite: |
  @James2021
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  #fig.width = 4, fig.height = 4,
  dpi = 150
)
```

```{r packages, include=FALSE}
library(tidyverse)
library(here)
library(glue)
library(patchwork)
library(gt)
```

```{r dunnr, include=FALSE}
library(dunnr)

# This command must be run once to register all fonts installed on Windows
# extrafont::font_import(pattern = "Roboto")
# This command must be run once in each R session to register fonts
extrafont::loadfonts(device = "win", quiet = TRUE)

theme_set(theme_td())
set_geom_fonts()
set_palette()
```

# 3.1 Simple Linear Regression

## 3.1.1 Estimating the Coefficients

To re-create Figure 3.1, import the `Advertising` data set:

```{r message=FALSE, warning=FALSE}
advertising <- read_csv(here("data", "Advertising.csv"))
glimpse(advertising)
```

Fit the simple linear model and draw the residuals to the line of best fit:

```{r}
lm_sales_tv <- lm(sales ~ TV, data = advertising)
advertising %>%
  bind_cols(
    pred_sales = predict(lm_sales_tv, data = advertising)
  ) %>%
  ggplot(aes(x = TV)) +
  geom_point(aes(y = sales), color = "red") +
  geom_smooth(aes(y = sales), method = "lm", formula = "y ~ x", se = FALSE) +
  geom_linerange(aes(ymin = sales, ymax = pred_sales))
```
```{r}
summary(lm_sales_tv)
```

We recover the same regression coefficients: $\beta_0$ = 7.03 and $\beta_1$ = 0.0475.

## 3.1.2 Assessing the Accuracy of the Coefficient Estimates

The quickest way to get the 95% confidence intervals for the coefficients is with `stats::confint()`:

```{r}
confint(lm_sales_tv)
```

Computing them manually requires the standard errors of the coefficients.
For this, I prefer `broom::tidy`:

```{r}
library(broom)
tidy(lm_sales_tv)
```

Then double the SEs to approximate the intervals:

```{r}
tidy(lm_sales_tv) %>%
  transmute(
    term, estimate,
    ci_lower = estimate - 2*std.error, ci_upper = estimate + 2*std.error
  )
```

The $t$-statistics are returned by `broom::tidy` as the `statistic` variable.
We can manually compute them with the estimates and SEs, and compute the $p$-value based on a $t$-distribution:

```{r}
tidy(lm_sales_tv) %>%
  transmute(
    term, estimate, se = std.error,
    t = estimate / se,
    p_value = 2 * pt(-t, df = nrow(advertising) - 2)
  )
```

## 3.1.3 Assessing the Accuracy of the Model

The `broom::glance` function gives summary statistics of a model:

```{r}
glance(lm_sales_tv)
```

The residual standard error (RSE) is `sigma`, variance explained $R^2$ is `r.squared`, and the $F$-statistic is `statistic`.
With this, we can re-create Table 3.2:

```{r}
glance(lm_sales_tv) %>%
  transmute(`Residual standard error` = round(sigma, 2),
            `R2` = round(r.squared, 3), `F-statistic` = round(statistic, 1)) %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = "Quantity", values_to = "Value") %>%
  gt()
```


# 3.2 Multiple Linear Regression

Before performing the multiple linear regression, fit the other two simple linear regression models on radio and newspaper budgets:

```{r}
lm_sales_radio <- lm(sales ~ radio, data = advertising)
lm_sales_newspaper <- lm(sales ~ newspaper, data = advertising)
```

The `gtsummary` package provides functions to quickly create regression summary tables, similar to Table 3.4:

```{r}
library(gtsummary)
tbl_regression(lm_sales_radio, intercept = TRUE)
tbl_regression(lm_sales_newspaper, intercept = TRUE)
```

Now fit the multiple regression model:

```{r}
lm_sales_mult <- lm(sales ~ TV + radio + newspaper, data = advertising)
tbl_regression(lm_sales_mult, intercept = TRUE)
```

The lack of association between sales and newspaper advertising can be explained by co-linearity with the other predictors, as shown in Table 3.5:

```{r}
advertising %>%
  pivot_longer(cols = -X1) %>%
  full_join(., ., by = "X1") %>%
  group_by(name.x, name.y) %>%
  summarise(
    corr_coef = cor(value.x, value.y) %>%
      round(4) %>%
      as.character(),
    .groups = "drop"
  ) %>%
  pivot_wider(names_from = name.y, values_from = corr_coef) %>%
  gt(rowname_col = "name.x")
```

Consider the hypothesis test:

$$
\begin{align}
H_0:& \beta_1 = \beta_2 = \dots = \beta_p = 0 \\
H_a:& \text{at least one of } \beta_j \text{ is non-zero.}
\end{align}
$$
This is performed by computing the $F$-statistic as in equation (3.23).
Instead of computing manually, we again use `broom::glance` to re-create Table 3.6:

```{r}
glance(lm_sales_mult) %>%
  transmute(`Residual standard error` = round(sigma, 2),
            `R2` = round(r.squared, 3), `F-statistic` = round(statistic, 1)) %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = "Quantity", values_to = "Value") %>%
  gt()
```

The value of 570 is far larger than 1, which is compelling evidence against the null $H_0$.
The $F$-statistic follows the $F$-distribution, so we can get a $p$-value using the values of $n$ and $p$.
Or automatically with `glance`:

```{r}
glance(lm_sales_mult) %>%
  select(statistic, p.value)
```

Another way to do this is to explicitly fit the null model (no predictors), and perform an analysis of variance with the two models using `anova`:

```{r}
lm_sales_null <- lm(sales ~ 1, data = advertising)
anova(lm_sales_null, lm_sales_mult)
```

This also makes it easy to compare models with different subsets of variables, as in equation (3.24).
For example, the model with and without `newspaper`:

```{r}
anova(
  lm(sales ~ TV + radio, data = advertising),
  lm_sales_mult
)
```

# 3.3 Other Considerations in the Regression Model

## 3.3.1 Qualitative Predictors

Load the `credit` data set and regress credit card balance on home ownership (two levels):

```{r}
credit <- ISLR2::Credit
lm_balance_own <- lm(Balance ~ Own, data = credit)
tbl_regression(lm_balance_own, intercept = TRUE,
               show_single_row = "Own")
```

And with region (three levels):

```{r}
lm_balance_region <- lm(Balance ~ Region, data = credit)
tbl_regression(lm_balance_region, intercept = TRUE)
```

Note that the model `summary()` returns the results of the $F$-test below:

```{r}
summary(lm_balance_region)
```

Which is the same as that returned by `anova()`:

```{r}
anova(lm_balance_region)
```

## 3.3.2 Extensions of the Linear Model

The effect on sales, with an interaction term between TV and radio:

```{r}
lm_sales_inter <- lm(sales ~ radio * TV, data = advertising)
tbl_regression(lm_sales_inter, intercept = TRUE)
```

Compare the model with and without the interaction term:

```{r}
lm_sales_radio_tv <- lm(sales ~ radio + TV, data = advertising)

glance(lm_sales_radio_tv) %>%
  mutate(model = "additive") %>%
  bind_rows(
    glance(lm_sales_inter) %>%
      mutate(model = "interaction")
  ) %>%
  select(model, r.squared, AIC, BIC)
```

Re-crate Figure 3.7, comparing the balance model with and without an interaction term:

```{r fig.width=6, fig.height=3}
lm_balance_income_student <-
  lm(Balance ~ Income + Student, data = credit)
lm_balance_income_student_inter <-
  lm(Balance ~ Income * Student, data = credit)

d <- tibble(Income = seq(0, 150, 0.1)) %>%
  crossing(Student = factor(c("No", "Yes")))
augment(lm_balance_income_student, newdata = d) %>%
  mutate(model = "additive") %>%
  bind_rows(
    augment(lm_balance_income_student_inter, newdata = d) %>%
      mutate(model = "interaction")
  ) %>%
  ggplot(aes(x = Income, y = .fitted, color = Student)) +
  geom_line() +
  facet_wrap(~model, nrow = 1) +
  labs(y = "Balance") +
  theme_td_grid()
```

Fit mpg to horsepower with a linear term, quadratic term, and up to the fifth degree:

```{r}
auto <- ISLR2::Auto

lm_mpg_hp <- lm(mpg ~ horsepower, data = auto)
lm_mpg_hp2 <- lm(mpg ~ horsepower + I(horsepower^2), data = auto)
lm_mpg_hp5 <-
  lm(
    mpg ~ horsepower + I(horsepower^2) + I(horsepower^3) +
      I(horsepower^4) + I(horsepower^5),
    data = auto
  )

d <- tibble(horsepower = seq(1, 250, 0.1))

augment(lm_mpg_hp, newdata = d) %>% mutate(model = "Linear") %>%
  bind_rows(
    augment(lm_mpg_hp2, newdata = d) %>% mutate(model = "Degree 2")
  ) %>%
  bind_rows(
    augment(lm_mpg_hp5, newdata = d) %>% mutate(model = "Degree 5")
  ) %>%
  ggplot(aes(x = horsepower, y = .fitted, color = model)) +
  geom_point(
    aes(y = mpg), data = auto, color = "lightgrey", shape = 21, size = 4
  ) +
  geom_line(size = 1.5) +
  coord_cartesian(xlim = c(40, 230), ylim = c(8, 52)) +
  add_facet_borders() +
  theme(legend.position = c(0.7, 0.8)) +
  labs(y = "mpg", color = NULL)
```

```{r}
glance(lm_mpg_hp) %>% mutate(model = "Linear") %>%
  bind_rows(
    glance(lm_mpg_hp2) %>% mutate(model = "Degree 2")
  ) %>%
  bind_rows(
    glance(lm_mpg_hp5) %>% mutate(model = "Degree 5")
  ) %>%
  select(model, r.squared, AIC, BIC) %>%
  gt()
```

## 3.3 Potential Problems

1. Non-linearity of the Data

```{r}
plot(lm_mpg_hp, 1)
plot(lm_mpg_hp2, 1)
```

2. Correlation of Error Terms

3. Non-constant Variance of Error Terms

```{r}
plot(lm_mpg_hp)
```

4. Outliers

5. High Leverage Points

6. Collinearity

```{r fig.width=5, fig.height=3}
credit %>%
  select(Limit, Age, Rating) %>%
  pivot_longer(cols = c(Age, Rating)) %>%
  ggplot(aes(x = Limit, y = value)) +
  geom_point() +
  facet_wrap(~name, nrow = 1, scales = "free_y")
```

```{r}
lm_balance_age_limit <- lm(Balance ~ Age + Limit, data = credit)
lm_balance_rating_limit <- lm(Balance ~ Rating + Limit, data = credit)

tbl_regression(lm_balance_age_limit)
tbl_regression(lm_balance_rating_limit)
```

To calculate variance inflation factors (VIF), there are R functions such as `car::vif` which can be used, but it is fairly simple to calculate by hand:

```{r}
lm_rating_age_limit <- lm(Rating ~ Age + Limit, data = credit)
lm_age_rating_limit <- lm(Age ~ Rating + Limit, data = credit)
lm_limit_age_rating <- lm(Limit ~ Age + Rating, data = credit)
tribble(
  ~Predictor, ~`R^2`,
  "Age", 1 / (1 - summary(lm_age_rating_limit)$r.squared),
  "Rating", 1 / (1 - summary(lm_rating_age_limit)$r.squared),
  "Limit", 1 / (1 - summary(lm_limit_age_rating)$r.squared)
) %>%
  gt()
```


# 3.4 The Marketing Plan

# 3.5 Comparison of Linear Regression with $K$-Nearest Neighbors

# 3.6 Lab: Linear Regression

## 3.6.1 Libraries

```{r}
boston <- ISLR2::Boston
```

## 3.6.2 Simple Linear Regression

Regression median value of owner-occupied homes `medv` on lower status of the population `lstat`:

```{r}
lm_medv_lstat <- lm(medv ~ lstat, data = boston)
confint(lm_medv_lstat)

nd <- tibble(lstat = c(5, 10, 15))

predict(lm_medv_lstat, nd, interval = "confidence")
predict(lm_medv_lstat, nd, interval = "prediction")

boston %>%
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm" , formula = "y ~ x")
```

To display model diagnostics, we can call `plot()` on the model object as we have before.
I like the `performance` package because it uses `ggplot2`:

```{r warning=FALSE, fig.width=9, fig.height=9}
performance::check_model(lm_medv_lstat)
```

## 3.6.3 Multiple Linear Regression

Fit to all predictors and check VIF:

```{r}
lm_medv_all <- lm(medv ~ ., data = boston)
performance::check_collinearity(lm_medv_all)
```

The `rad` (accessibility to radial highways) and `tax` (property tax rate) variables have moderate VIF.

## 3.6.4 Interaction Terms

## 3.6.5 Non-linear Transformations of the Predictors

Perform a regression of `medv` onto `lstat` and `lstat^2`, and compare with `anova`:

```{r}
lm_medv_lstat2 <- lm(medv ~ lstat + I(lstat^2), data = boston)
tbl_regression(lm_medv_lstat2)

anova(lm_medv_lstat, lm_medv_lstat2)
```

## 3.6.6 Qualitative Predictors

```{r}
carseats <- ISLR2::Carseats
lm_sales <- lm(Sales ~ . + Income:Advertising + Price:Age,
               data = carseats)

contrasts(carseats$ShelveLoc)
```

# 3.7 Exercises

I'll attempt to do these exercises in the `tidymodels` framework.

## 8. Simple linear regression with `Auto`

* Fit the model and display the summary

This is way overkill for a simple linear regression, but here is a `tidymodels` workflow object for regressing `mpg` on `horsepower`:

```{r}
lm_mpg_hp_recipe <- recipe(mpg ~ horsepower, data = auto)
lm_mpg_hp_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")
lm_mpg_hp_workflow <- workflow() %>%
  add_recipe(lm_mpg_hp_recipe) %>%
  add_model(lm_mpg_hp_spec)
lm_mpg_hp_workflow
```

Fit the model and print the estimates:

```{r}
lm_mpg_hp_fit <- lm_mpg_hp_workflow %>%
  fit(data = auto)
tidy(lm_mpg_hp_fit)
```



```{r}
summary(lm_mpg_hp_fit$fit)
```



    * There is a relationship between `mpg` and `horsepower`.
    * Is is highly significant: $p$ <2e-16.
    * The relationship is negative. Every unit of horsepower is associated with a -0.158 reduction in miles per gallon.
    * The confidence and prediction intervals of predicted `mpg` given `horsepower` = 98:

```{r}
predict(lm_mpg_hp_fit, tibble(horsepower = 98), type = "conf_int")
predict(lm_mpg_hp_fit, tibble(horsepower = 98), type = "pred_int")
```

Note that the arguments to `predict()` have to be altered when using a `tidymodels` fit.
It also doesn't return the point estimate.

* Plot with best fit line:

```{r warning=FALSE}
auto %>%
  ggplot(aes(x = horsepower)) +
  geom_point(aes(y = mpg)) +
  geom_line(
    data = augment(lm_mpg_hp_fit, tibble(horsepower = 50:220)),
    aes(y = .pred), size = 2, color = td_colors$nice$emerald
  )
```

* Diagnostic plots:

Do use the convenient `plot()` or `performance::check_model()` plotting functions, we need to extract the actual `lm` object from the `tidymodels` fit:

```{r fig.height=7, fig.width=6, warning=FALSE}
lm_mpg_hp_fit %>%
  extract_fit_engine() %>%
  performance::check_model()
```

Two potential problems: non-linearity (top left plot) and homogeneity of variance (top right).

## 9. Multiple linear regression with `Auto`

* Scatterplot of all variables.

There are some advanced ways to produce these bivariate correlation plots in `gglpot2`:

```{r fig.height=8, fig.width=8}
auto %>%
  select(-name) %>%
  rowid_to_column() %>%
  pivot_longer(cols = -rowid) %>%
  mutate(name = factor(name)) %>%
  full_join(., ., by = "rowid") %>%
  filter(as.integer(name.x) > as.integer(name.y)) %>%
  ggplot(aes(x = value.x, y = value.y)) +
  geom_point() +
  facet_wrap(name.x ~ name.y, scales = "free") +
  add_facet_borders()
```

But they take a lot of effort to look nice.
The `GGally::ggpairs()` function quickly makes a scatterplot matrix from a given data set, and also gives the correlation coefficients:

```{r fig.height=7, fig.width=7}
GGally::ggpairs(auto %>% select(-name))
```

* Compute the correlations.

```{r}
cor(auto %>% select(-name))
```

* Fit the multiple linear regression.

```{r}
lm_mpg_recipe <- recipe(mpg ~ ., data = auto) %>%
  step_rm(name)

# Skip the spec step, and just put it directly into the workflow
lm_mpg_workflow <- workflow() %>%
  add_recipe(lm_mpg_recipe) %>%
  # By default, linear_reg() will use lm as the engine and regression as mode
  add_model(linear_reg())
lm_mpg_workflow

lm_mpg_fit <- lm_mpg_workflow %>%
  fit(data = auto)
tidy(lm_mpg_fit)
```


* There is a relationship between the predictors and `mpg`: $F$ = `r summary(lm_mpg)$fstatistic[1] %>% round(2)`
* The following terms are statistically significant: `r tidy(lm_mpg) %>% filter(p.value < 0.05) %>% pull(term) %>% str_c(collapse = ", ")`
* The coefficient for `year` suggests that, for every increment in car model year, `mpg` increases by `r tidy(lm_mpg) %>% filter(term == "year") %>% pull(estimate) %>% round(2)`
        
```{r fig.height=7, fig.width=7, warning=FALSE}
performance::check_model(lm_mpg)
```

Some non-linearity and collinearity.
There is a point with high leverage, but it has a low residual.

## 10. Multiple linear regression with `Carseats`

```{r}
lm_sales_price_urban_us <- lm(Sales ~ Price + Urban + US, data = carseats)
summary(lm_sales_price_urban_us)
```


# Reproducibility

<details><summary>Reproducibility receipt</summary>

```{r}
Sys.time()
```

```{r}
if ("git2r" %in% installed.packages()) {
  if (git2r::in_repository()) {
    git2r::repository()
  }
}
```

```{r}
sessioninfo::session_info()
```

</details>

# References
